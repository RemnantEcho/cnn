{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import zipfile as zp\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "# with open('t10k-images-idx3-ubyte', 'rb') as f_in:\n",
    "#     with gzip.open('t10k-images-idx3-ubyte.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "num_of_labels = 10\n",
    "\n",
    "# Assign Zip Files to variable\n",
    "zf = zp.ZipFile('fashion-mnist_train.zip')\n",
    "zf1 = zp.ZipFile('fashion-mnist_test.zip')\n",
    "\n",
    "# Load CSV files from Zip Files\n",
    "train_data = np.loadtxt(zf.open('fashion-mnist_train.csv'), delimiter=',')\n",
    "test_data = np.loadtxt(zf1.open('fashion-mnist_test.csv'), delimiter=',')\n",
    "\n",
    "# Display images from 1 to 10\n",
    "# for i in range(10):\n",
    "#     img = train_imgs[i].reshape((28,28))\n",
    "#     plt.imshow(img, cmap=\"Greys\")\n",
    "#     plt.show()\n",
    "\n",
    "# Map image data values into intervals [0.01, 0.99]\n",
    "fac = 0.99 / 255\n",
    "add_fac = 0.01\n",
    "train_imgs = np.asfarray(train_data[:, 1:], dtype='float') * fac + add_fac\n",
    "test_imgs = np.asfarray(test_data[:, 1:], dtype='float') *fac + add_fac\n",
    "train_labels = np.asfarray(train_data[:, :1], dtype='float')\n",
    "test_labels = np.asfarray(test_data[:, :1], dtype='float')\n",
    "\n",
    "lr = np.arange(num_of_labels)\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(float)\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n",
    "\n",
    "# Create Pickle file from previous data\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"bw\") as fh:\n",
    "    data = (train_imgs, \n",
    "            test_imgs, \n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            train_labels_one_hot,\n",
    "            test_labels_one_hot)\n",
    "    pickle.dump(data, fh)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    row = len(x)\n",
    "    column = len(x[0])\n",
    "    \n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            if x[r, c]:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "def softmax(x):\n",
    "    assert len(x.shape) == 2\n",
    "    s = np.max(x, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(x - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def leaky_relu(x):\n",
    "    _x = x.copy()\n",
    "    _x[x < 0] = _x[x < 0] * 0.01\n",
    "    return _x\n",
    "\n",
    "def dleaky_relu(x):\n",
    "    out = np.ones_like(x)\n",
    "    out[x < 0] *= 0.01\n",
    "    return out\n",
    "\n",
    "def dlrelu(x, alpha=0.01):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0] = alpha\n",
    "    return dx\n",
    "            \n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def dsigmoid(x):\n",
    "    output = 1/(1+np.e ** -x)\n",
    "    return output * (1 - output)\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd, \n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 activation_function,\n",
    "                 learning_rate):\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.learning_rate = learning_rate \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = dsigmoid\n",
    "            \n",
    "        if activation_function == 'softmax':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = softmax\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation = relu\n",
    "            self.dactivation = drelu\n",
    "            \n",
    "        if activation_function == 'leakyrelu':\n",
    "            self.activation = leaky_relu\n",
    "            self.dactivation = softmax\n",
    "        \n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
    "        X = truncated_normal(mean=0, \n",
    "                             sd=1, \n",
    "                             low=-rad, \n",
    "                             upp=rad)\n",
    "        self.wih = X.rvs((self.no_of_hidden_nodes, \n",
    "                                       self.no_of_in_nodes))\n",
    "        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
    "        X = truncated_normal(mean=0, \n",
    "                             sd=1, \n",
    "                             low=-rad, \n",
    "                             upp=rad)\n",
    "        self.who = X.rvs((self.no_of_out_nodes, \n",
    "                                        self.no_of_hidden_nodes))\n",
    "        \n",
    "    \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        \"\"\"\n",
    "        input_vector and target_vector can be tuple, \n",
    "        list or ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        output_vectors = []\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "        \n",
    "        output_vector1 = np.dot(self.wih, \n",
    "                                input_vector)\n",
    "        output_hidden = self.activation(output_vector1)\n",
    "        \n",
    "        output_vector2 = np.dot(self.who, \n",
    "                                output_hidden)\n",
    "        \n",
    "        output_network = self.dactivation(output_vector2)\n",
    "        \n",
    "        output_errors = target_vector - output_network\n",
    "        # update the weights:\n",
    "        tmp = output_errors * output_network * \\\n",
    "              (1.0 - output_network)     \n",
    "        tmp = self.learning_rate  * np.dot(tmp, \n",
    "                                           output_hidden.T)\n",
    "        self.who += tmp\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.who.T, \n",
    "                               output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * output_hidden * (1.0 - output_hidden)\n",
    "        self.wih += self.learning_rate * np.dot(tmp, input_vector.T)\n",
    "        \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              intermediate_results=False):\n",
    "        intermediate_weights = []\n",
    "        for epoch in range(epochs):  \n",
    "            print(\"*\", end=\"\")\n",
    "            for i in range(len(data_array)):\n",
    "                self.train_single(data_array[i], \n",
    "                                  labels_one_hot_array[i])\n",
    "            if intermediate_results:\n",
    "                intermediate_weights.append((self.wih.copy(), \n",
    "                                             self.who.copy()))\n",
    "        return intermediate_weights        \n",
    "            \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = {}\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            if (target, res_max) in cm:\n",
    "                cm[(target, res_max)] += 1\n",
    "            else:\n",
    "                cm[(target, res_max)] = 1\n",
    "        return cm\n",
    "        \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        \"\"\" input_vector can be tuple, list or ndarray \"\"\"\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        output_vector = np.dot(self.wih, \n",
    "                               input_vector)\n",
    "        output_vector = self.activation(output_vector)\n",
    "        \n",
    "        output_vector = np.dot(self.who, \n",
    "                               output_vector)\n",
    "        output_vector = self.activation(output_vector)\n",
    "    \n",
    "        return output_vector\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********epoch:  0\n",
      "accuracy train:  0.29861666666666664\n",
      "accuracy test:  0.2947\n",
      "epoch:  1\n",
      "accuracy train:  0.31995\n",
      "accuracy test:  0.3149\n",
      "epoch:  2\n",
      "accuracy train:  0.23391666666666666\n",
      "accuracy test:  0.23\n",
      "epoch:  3\n",
      "accuracy train:  0.17453333333333335\n",
      "accuracy test:  0.1714\n",
      "epoch:  4\n",
      "accuracy train:  0.1683\n",
      "accuracy test:  0.1646\n",
      "epoch:  5\n",
      "accuracy train:  0.09906666666666666\n",
      "accuracy test:  0.0995\n",
      "epoch:  6\n",
      "accuracy train:  0.09978333333333333\n",
      "accuracy test:  0.1002\n",
      "epoch:  7\n",
      "accuracy train:  0.10015\n",
      "accuracy test:  0.1007\n",
      "epoch:  8\n",
      "accuracy train:  0.10556666666666667\n",
      "accuracy test:  0.1048\n",
      "epoch:  9\n",
      "accuracy train:  0.10351666666666667\n",
      "accuracy test:  0.1029\n"
     ]
    }
   ],
   "source": [
    "# Load data from Pickle file\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "train_labels_one_hot = data[4]\n",
    "test_labels_one_hot = data[5]\n",
    "\n",
    "img_size = 28 # dimensions\n",
    "num_of_labels = 10 # 0, 1, 2, ... 9\n",
    "image_pixels = img_size * img_size\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "ANN = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                    no_of_out_nodes = 10, \n",
    "                    no_of_hidden_nodes = 100,\n",
    "                    activation_function = 'relu',\n",
    "                    learning_rate = 0.15)\n",
    "    \n",
    "    \n",
    " \n",
    "weights = ANN.train(train_imgs, \n",
    "                    train_labels_one_hot, \n",
    "                    epochs=epochs, \n",
    "                    intermediate_results=True)\n",
    "\n",
    "for i in range(epochs):  \n",
    "    print(\"epoch: \", i)\n",
    "    ANN.wih = weights[i][0]\n",
    "    ANN.who = weights[i][1]\n",
    "   \n",
    "    corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
    "    print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
    "    corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
    "    print(\"accuracy test: \", corrects / ( corrects + wrongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
