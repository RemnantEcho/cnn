{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jly09\\AppData\\Local\\Temp\\ipykernel_16840\\3536206702.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
      "C:\\Users\\jly09\\AppData\\Local\\Temp\\ipykernel_16840\\3536206702.py:45: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_labels_one_hot = (lr==test_labels).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import zipfile as zp\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "# with open('t10k-images-idx3-ubyte', 'rb') as f_in:\n",
    "#     with gzip.open('t10k-images-idx3-ubyte.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "num_of_labels = 10\n",
    "\n",
    "# Assign Zip Files to variable\n",
    "zf = zp.ZipFile('fashion-mnist_train.zip')\n",
    "zf1 = zp.ZipFile('fashion-mnist_test.zip')\n",
    "\n",
    "# Load CSV files from Zip Files\n",
    "train_data = np.loadtxt(zf.open('fashion-mnist_train.csv'), delimiter=',')\n",
    "test_data = np.loadtxt(zf1.open('fashion-mnist_test.csv'), delimiter=',')\n",
    "\n",
    "# Display images from 1 to 10\n",
    "# for i in range(10):\n",
    "#     img = train_imgs[i].reshape((28,28))\n",
    "#     plt.imshow(img, cmap=\"Greys\")\n",
    "#     plt.show()\n",
    "\n",
    "# Map image data values into intervals [0.01, 0.99]\n",
    "fac = 0.99 / 255\n",
    "add_fac = 0.01\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + add_fac\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) *fac + add_fac\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "lr = np.arange(num_of_labels)\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(float)\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n",
    "\n",
    "# Create Pickle file from previous data\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"bw\") as fh:\n",
    "    data = (train_imgs, \n",
    "            test_imgs, \n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            train_labels_one_hot,\n",
    "            test_labels_one_hot)\n",
    "    pickle.dump(data, fh)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    row = len(x)\n",
    "    column = len(x[0])\n",
    "    \n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            if x[r, c]:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "def softmax(x):\n",
    "    assert len(x.shape) == 2\n",
    "    s = np.max(x, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(x - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def leaky_relu(x):\n",
    "    _x = x.copy()\n",
    "    _x[x < 0] = _x[x < 0] * 0.01\n",
    "    return _x\n",
    "\n",
    "def dleaky_relu(x):\n",
    "    out = np.ones_like(x)\n",
    "    out[x < 0] *= 0.01\n",
    "    return out\n",
    "            \n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def dsigmoid(x):\n",
    "    output = 1/(1+np.e ** -x)\n",
    "    return output * (1 - output)\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd, \n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 activation_function,\n",
    "                 learning_rate,\n",
    "                 bias=None,\n",
    "                 active_input_percentage=0.70,\n",
    "                 active_hidden_percentage=0.70\n",
    "                ):  \n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes       \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.active_input_percentage=active_input_percentage\n",
    "        self.active_hidden_percentage=active_input_percentage\n",
    "        \n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = dsigmoid\n",
    "            \n",
    "        if activation_function == 'softmax':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = softmax\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation = relu\n",
    "            self.dactivation = drelu\n",
    "            \n",
    "        if activation_function == 'leakyrelu':\n",
    "            self.activation = leaky_relu\n",
    "            self.dactivation = dleaky_relu\n",
    "            \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "#         \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "#         rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
    "#         X = truncated_normal(mean=0, \n",
    "#                              sd=1, \n",
    "#                              low=-rad, \n",
    "#                              upp=rad)\n",
    "#         self.wih = X.rvs((self.no_of_hidden_nodes, \n",
    "#                                        self.no_of_in_nodes))\n",
    "#         rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
    "#         X = truncated_normal(mean=0, \n",
    "#                              sd=1, \n",
    "#                              low=-rad, \n",
    "#                              upp=rad)\n",
    "#         self.who = X.rvs((self.no_of_out_nodes, \n",
    "#                                         self.no_of_hidden_nodes))\n",
    "\n",
    "        bias_node = 1 if self.bias else 0\n",
    "        n = (self.no_of_in_nodes + bias_node) * self.no_of_hidden_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wih = X.rvs(n).reshape((self.no_of_hidden_nodes, \n",
    "                                                   self.no_of_in_nodes + bias_node))\n",
    "        n = (self.no_of_hidden_nodes + bias_node) * self.no_of_out_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.who = X.rvs(n).reshape((self.no_of_out_nodes, \n",
    "                                                    (self.no_of_hidden_nodes + bias_node)))\n",
    "        \n",
    "    def dropout_weight_matrices(self):\n",
    "        # restore wih array, if it had been used for dropout\n",
    "        self.wih_orig = self.wih.copy()\n",
    "        self.no_of_in_nodes_orig = self.no_of_in_nodes\n",
    "        self.no_of_hidden_nodes_orig = self.no_of_hidden_nodes\n",
    "        self.who_orig = self.who.copy()\n",
    "        \n",
    "        active_input_nodes = int(self.no_of_in_nodes * self.active_input_percentage)\n",
    "        active_input_indices = sorted(random.sample(range(0, self.no_of_in_nodes), \n",
    "                                      active_input_nodes))\n",
    "        active_hidden_nodes = int(self.no_of_hidden_nodes * self.active_hidden_percentage)\n",
    "        active_hidden_indices = sorted(random.sample(range(0, self.no_of_hidden_nodes), \n",
    "                                       active_hidden_nodes))\n",
    "        \n",
    "        self.wih = self.wih[:, active_input_indices][active_hidden_indices]       \n",
    "        self.who = self.who[:, active_hidden_indices]\n",
    "        \n",
    "        self.no_of_hidden_nodes = active_hidden_nodes\n",
    "        self.no_of_in_nodes = active_input_nodes\n",
    "        return active_input_indices, active_hidden_indices\n",
    "    \n",
    "    def weight_matrices_reset(self, \n",
    "                              active_input_indices, \n",
    "                              active_hidden_indices):\n",
    "        \n",
    "        \"\"\"\n",
    "        self.wih and self.who contain the newly adapted values from the active nodes.\n",
    "        We have to reconstruct the original weight matrices by assigning the new values \n",
    "        from the active nodes\n",
    "        \"\"\"\n",
    " \n",
    "        temp = self.wih_orig.copy()[:,active_input_indices]\n",
    "        temp[active_hidden_indices] = self.wih\n",
    "        self.wih_orig[:, active_input_indices] = temp\n",
    "        self.wih = self.wih_orig.copy()\n",
    "        self.who_orig[:, active_hidden_indices] = self.who\n",
    "        self.who = self.who_orig.copy()\n",
    "        self.no_of_in_nodes = self.no_of_in_nodes_orig\n",
    "        self.no_of_hidden_nodes = self.no_of_hidden_nodes_orig\n",
    "        \n",
    "    \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        \"\"\" \n",
    "        input_vector and target_vector can be tuple, list or ndarray\n",
    "        \"\"\"\n",
    " \n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "        output_vector1 = np.dot(self.wih, input_vector)\n",
    "        output_vector_hidden = self.activation(output_vector1)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
    "        \n",
    "        output_vector2 = np.dot(self.who, output_vector_hidden)\n",
    "        output_vector_network = self.dactivation(output_vector2)\n",
    "        \n",
    "        output_errors = target_vector - output_vector_network\n",
    "        # update the weights:\n",
    "        tmp = output_errors * output_vector_network * (1.0 - output_vector_network) / self.active_input_percentage  \n",
    "        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden.T)\n",
    "        self.who += tmp\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden) / self.active_hidden_percentage\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, input_vector.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, input_vector.T)\n",
    "        self.wih += self.learning_rate * x\n",
    "        \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              active_input_percentage=0.70,\n",
    "              active_hidden_percentage=0.70,\n",
    "              no_of_dropout_tests = 10):\n",
    "        partition_length = int(len(data_array) / no_of_dropout_tests)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch: \", epoch)\n",
    "            for start in range(0, len(data_array), partition_length):\n",
    "                active_in_indices, active_hidden_indices = \\\n",
    "                           self.dropout_weight_matrices()\n",
    "                for i in range(start, start + partition_length):\n",
    "                    self.train_single(data_array[i][active_in_indices], \n",
    "                                     labels_one_hot_array[i]) \n",
    "                    \n",
    "                self.weight_matrices_reset(active_in_indices, active_hidden_indices)        \n",
    "            \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = {}\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            if (target, res_max) in cm:\n",
    "                cm[(target, res_max)] += 1\n",
    "            else:\n",
    "                cm[(target, res_max)] = 1\n",
    "        return cm\n",
    "        \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        \n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        #wih = self.wih * self.active_input_percentage\n",
    "        \n",
    "        output_vector = np.dot(self.wih, input_vector)\n",
    "        output_vector = self.activation(output_vector)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[self.bias]]) )\n",
    "        #who = self.who * self.active_hidden_percentage    \n",
    "        \n",
    "        output_vector = np.dot(self.who, output_vector)\n",
    "        output_vector = self.dactivation(output_vector)\n",
    "        \n",
    "        \n",
    "        return output_vector\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "accruracy train:  0.006716666666666667\n",
      "accruracy: test 0.0081\n"
     ]
    }
   ],
   "source": [
    "# Load data from Pickle file\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "train_labels_one_hot = data[4]\n",
    "test_labels_one_hot = data[5]\n",
    "\n",
    "img_size = 28 # dimensions\n",
    "num_of_labels = 10 # 0, 1, 2, ... 9\n",
    "image_pixels = img_size * img_size\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "simple_network = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                               no_of_out_nodes = 10, \n",
    "                               no_of_hidden_nodes = 100,\n",
    "                               activation_function = 'sigmoid',\n",
    "                               learning_rate = 0.1)\n",
    "    \n",
    " \n",
    "simple_network.train(train_imgs, \n",
    "                     train_labels_one_hot, \n",
    "                     active_input_percentage=1,\n",
    "                     active_hidden_percentage=1,\n",
    "                     no_of_dropout_tests = 100,\n",
    "                     epochs=epochs)\n",
    "\n",
    "corrects, wrongs = simple_network.evaluate(train_imgs, train_labels)\n",
    "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = simple_network.evaluate(test_imgs, test_labels)\n",
    "print(\"accuracy: test\", corrects / ( corrects + wrongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
