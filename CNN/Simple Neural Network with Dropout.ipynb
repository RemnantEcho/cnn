{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3063 Mathematics and Programming for AI\n",
    "## Resit Coursework\n",
    "\n",
    "### 1. Introduction\n",
    "This program is a simple Neural Network that evaluates the accuracy of a training set and testing set, with the implementation of three kinds of activation methods (Sigmoid, ReLu, LeakyReLu), Softmax and Inverted Dropout.\n",
    "\n",
    "This program is made using Python 3 through Jupyter Notebook IDE. \n",
    "\n",
    "### 2. Instructions\n",
    "\n",
    "Best way to run this program is to restart the whole kernel, as the cells follow a sequential routine.\n",
    "\n",
    "For re-running the program without restarting the whole kernel, after the initial run, run the cells from Loading Data and Training NN and below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import Libraries\n",
    "\n",
    "Necessary libraries that are used for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import zipfile as zp\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Data and Pickling\n",
    "\n",
    "Extract compressed datasets from Zip Files and then proceed to package the data together into a pickle (.pkl) file for easier extraction of data.\n",
    "\n",
    "###### Make sure to run this at least once before running the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('t10k-images-idx3-ubyte', 'rb') as f_in:\n",
    "#     with gzip.open('t10k-images-idx3-ubyte.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "num_of_labels = 10\n",
    "\n",
    "# Assign Zip Files to variable\n",
    "zf = zp.ZipFile('fashion-mnist_train.zip')\n",
    "zf1 = zp.ZipFile('fashion-mnist_test.zip')\n",
    "\n",
    "# Load CSV files from Zip Files\n",
    "train_data = np.loadtxt(zf.open('fashion-mnist_train.csv'), delimiter=',')\n",
    "test_data = np.loadtxt(zf1.open('fashion-mnist_test.csv'), delimiter=',')\n",
    "\n",
    "# Display images from 1 to 10\n",
    "# for i in range(10):\n",
    "#     img = train_imgs[i].reshape((28,28))\n",
    "#     plt.imshow(img, cmap=\"Greys\")\n",
    "#     plt.show()\n",
    "\n",
    "# Map image data values into intervals [0.01, 0.99]\n",
    "fac = 0.99 / 255\n",
    "add_fac = 0.01\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + add_fac\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) *fac + add_fac\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "lr = np.arange(num_of_labels)\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n",
    "\n",
    "# Create Pickle file from previous data\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"bw\") as fh:\n",
    "    data = (train_imgs, \n",
    "            test_imgs, \n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            train_labels_one_hot,\n",
    "            test_labels_one_hot)\n",
    "    pickle.dump(data, fh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Activation Functions, Softmax, and NN Class\n",
    "#### Task 1, 2, 3, 4\n",
    "\n",
    "We have the three activation functions: sigmoid, relu, and leakyrelu. Softmax function necessary for Task 3. Neural Network class with selectable activation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    row = len(x)\n",
    "    column = len(x[0])\n",
    "    \n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            if x[r, c]:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "def softmax(x):\n",
    "    assert len(x.shape) == 2\n",
    "    s = np.max(x, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(x - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def leaky_relu(x):\n",
    "    _x = x.copy()\n",
    "    _x[x < 0] = _x[x < 0] * 0.01\n",
    "    return _x\n",
    "\n",
    "def dleaky_relu(x):\n",
    "    _x = x.copy()\n",
    "    row = len(_x)\n",
    "    column = len(_x[0])\n",
    "    \n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            if _x[r, c] < 0:\n",
    "                _x[r, c] = _x[r, c] * 0.01\n",
    "                \n",
    "    return _x\n",
    "            \n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def dsigmoid(x):\n",
    "    output = 1/(1+np.e ** -x)\n",
    "    return output * (1 - output)\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd, \n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 activation_function,\n",
    "                 learning_rate,\n",
    "                 bias=None,\n",
    "                 active_input_percentage=0.70,\n",
    "                 active_hidden_percentage=0.70\n",
    "                ):  \n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes       \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.active_input_percentage=active_input_percentage\n",
    "        self.active_hidden_percentage=active_input_percentage\n",
    "        \n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = dsigmoid\n",
    "            \n",
    "        if activation_function == 'softmax':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = softmax\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation = relu\n",
    "            self.dactivation = drelu\n",
    "            \n",
    "        if activation_function == 'leakyrelu':\n",
    "            self.activation = leaky_relu\n",
    "            self.dactivation = dleaky_relu\n",
    "            \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "#         \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "#         rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
    "#         X = truncated_normal(mean=0, \n",
    "#                              sd=1, \n",
    "#                              low=-rad, \n",
    "#                              upp=rad)\n",
    "#         self.wih = X.rvs((self.no_of_hidden_nodes, \n",
    "#                                        self.no_of_in_nodes))\n",
    "#         rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
    "#         X = truncated_normal(mean=0, \n",
    "#                              sd=1, \n",
    "#                              low=-rad, \n",
    "#                              upp=rad)\n",
    "#         self.who = X.rvs((self.no_of_out_nodes, \n",
    "#                                         self.no_of_hidden_nodes))\n",
    "\n",
    "        bias_node = 1 if self.bias else 0\n",
    "        n = (self.no_of_in_nodes + bias_node) * self.no_of_hidden_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wih = X.rvs(n).reshape((self.no_of_hidden_nodes, \n",
    "                                                   self.no_of_in_nodes + bias_node))\n",
    "        n = (self.no_of_hidden_nodes + bias_node) * self.no_of_out_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.who = X.rvs(n).reshape((self.no_of_out_nodes, \n",
    "                                                    (self.no_of_hidden_nodes + bias_node)))\n",
    "        \n",
    "    def dropout_weight_matrices(self):\n",
    "        # restore wih array, if it had been used for dropout\n",
    "        self.wih_orig = self.wih.copy()\n",
    "        self.no_of_in_nodes_orig = self.no_of_in_nodes\n",
    "        self.no_of_hidden_nodes_orig = self.no_of_hidden_nodes\n",
    "        self.who_orig = self.who.copy()\n",
    "        \n",
    "        active_input_nodes = int(self.no_of_in_nodes * self.active_input_percentage)\n",
    "        active_input_indices = sorted(random.sample(range(0, self.no_of_in_nodes), \n",
    "                                      active_input_nodes))\n",
    "        active_hidden_nodes = int(self.no_of_hidden_nodes * self.active_hidden_percentage)\n",
    "        active_hidden_indices = sorted(random.sample(range(0, self.no_of_hidden_nodes), \n",
    "                                       active_hidden_nodes))\n",
    "        \n",
    "        self.wih = self.wih[:, active_input_indices][active_hidden_indices]       \n",
    "        self.who = self.who[:, active_hidden_indices]\n",
    "        \n",
    "        self.no_of_hidden_nodes = active_hidden_nodes\n",
    "        self.no_of_in_nodes = active_input_nodes\n",
    "        return active_input_indices, active_hidden_indices\n",
    "    \n",
    "    def weight_matrices_reset(self, \n",
    "                              active_input_indices, \n",
    "                              active_hidden_indices):\n",
    "        \n",
    "        \"\"\"\n",
    "        self.wih and self.who contain the newly adapted values from the active nodes.\n",
    "        We have to reconstruct the original weight matrices by assigning the new values \n",
    "        from the active nodes\n",
    "        \"\"\"\n",
    " \n",
    "        temp = self.wih_orig.copy()[:,active_input_indices]\n",
    "        temp[active_hidden_indices] = self.wih\n",
    "        self.wih_orig[:, active_input_indices] = temp\n",
    "        self.wih = self.wih_orig.copy()\n",
    "        self.who_orig[:, active_hidden_indices] = self.who\n",
    "        self.who = self.who_orig.copy()\n",
    "        self.no_of_in_nodes = self.no_of_in_nodes_orig\n",
    "        self.no_of_hidden_nodes = self.no_of_hidden_nodes_orig\n",
    "        \n",
    "    \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        \"\"\" \n",
    "        input_vector and target_vector can be tuple, list or ndarray\n",
    "        \"\"\"\n",
    "         # Forward Pass\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "        output_vector1 = np.dot(self.wih, input_vector)\n",
    "        output_vector_hidden = self.activation(output_vector1)\n",
    "        \n",
    "        # Backward Pass\n",
    "        if self.bias:\n",
    "            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
    "        \n",
    "        output_vector2 = np.dot(self.who, output_vector_hidden)\n",
    "        output_vector_network = self.dactivation(output_vector2)\n",
    "        \n",
    "        output_errors = target_vector - output_vector_network\n",
    "        # update the weights:\n",
    "        # inverted dropout\n",
    "        tmp = output_errors * output_vector_network * (1.0 - output_vector_network) / self.active_input_percentage  \n",
    "        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden.T)\n",
    "        self.who += tmp\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden) / self.active_hidden_percentage\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, input_vector.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, input_vector.T)\n",
    "        self.wih += self.learning_rate * x\n",
    "        \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              active_input_percentage=0.70,\n",
    "              active_hidden_percentage=0.70,\n",
    "              no_of_dropout_tests = 10):\n",
    "        partition_length = int(len(data_array) / no_of_dropout_tests)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch: \", epoch)\n",
    "            for start in range(0, len(data_array), partition_length):\n",
    "                active_in_indices, active_hidden_indices = \\\n",
    "                           self.dropout_weight_matrices()\n",
    "                for i in range(start, start + partition_length):\n",
    "                    self.train_single(data_array[i][active_in_indices], \n",
    "                                     labels_one_hot_array[i]) \n",
    "                    \n",
    "                self.weight_matrices_reset(active_in_indices, active_hidden_indices)        \n",
    "            \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = {}\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            if (target, res_max) in cm:\n",
    "                cm[(target, res_max)] += 1\n",
    "            else:\n",
    "                cm[(target, res_max)] = 1\n",
    "        return cm\n",
    "        \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        \n",
    "        # Forward Pass\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        #wih = self.wih * self.active_input_percentage\n",
    "        \n",
    "        output_vector = np.dot(self.wih, input_vector)\n",
    "        output_vector = self.activation(output_vector)\n",
    "        \n",
    "        # Backward Pass\n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[self.bias]]) )\n",
    "        #who = self.who * self.active_hidden_percentage    \n",
    "        \n",
    "        output_vector = np.dot(self.who, output_vector)\n",
    "        output_vector = self.dactivation(output_vector)\n",
    "        \n",
    "        \n",
    "        return output_vector\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs\n",
    "    \n",
    "     def plot_graph(self):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.use_sticky_edges = False\n",
    "        ax.scatter(self.data_plots_epoch, self.data_plots_train, color = 'red', label = 'train')\n",
    "        ax.scatter(self.data_plots_epoch, self.data_plots_test, color = 'blue', label = 'test')\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "\n",
    "        loc = plticker.MultipleLocator(base=1.0)\n",
    "        ax.xaxis.set_major_locator(loc)\n",
    "        loc = plticker.MultipleLocator(base=0.1)\n",
    "        ax.yaxis.set_major_locator(loc)\n",
    "\n",
    "        ax.set_ylim(0,1)\n",
    "        for i,j in zip(self.data_plots_epoch, self.data_plots_train):\n",
    "            ax.annotate(str(j),xy=(i + 0.01,j + 0.04))\n",
    "\n",
    "        for i,j in zip(self.data_plots_epoch, self.data_plots_test):\n",
    "            ax.annotate(str(j),xy=(i + 0.01,j + 0.04))\n",
    "\n",
    "        title_act = ''\n",
    "\n",
    "        if self.activation_function == Sigmoid:\n",
    "            title_act = 'Sigmoid'\n",
    "\n",
    "        if self.activation_function == relu:\n",
    "            title_act = 'ReLU'\n",
    "\n",
    "        if self.activation_function == leaky_relu:\n",
    "            title_act = 'Leaky ReLU'\n",
    "\n",
    "        ax.set_title(self.ds_name + ' Accuracy using ' + title_act)\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "       \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Loading data and Training NN\n",
    "\n",
    "Load the data from the pickle file, initialise the Neural Network with the desired arguments and then train them.\n",
    "\n",
    "Notable arguments:\n",
    "\n",
    "    no_of_in_nodes - original image data size (28x28) being passed through equals input size\n",
    "    no_of_out_nodes - desired amount of output nodes for the data\n",
    "    no_of_hidden_nodes - desired number of hidden nodes to process the data\n",
    "    learning_rate - the desired learning rate\n",
    "    activation_function - the desired activation functions; \n",
    "        includes: Sigmoid 'sigmoid', ReLU 'relu', and Leaky ReLU 'leakyrelu'\n",
    "\n",
    "To use different activation functions for the Neural Network, initialise a new NeuralNetwork class and set the value of the activation_function variable as 'sigoid' for Sigmoid, 'relu' for ReLu, and 'leakyrelu' for LeakyReLu.\n",
    "\n",
    "For Softmax pass the activation_funciton variable, when initialising the Neural Network, to 'softmax'.\n",
    "\n",
    "To use dropout, when running the Train change the ative_input_percentage, active_hidden_percentage, and no_of_dropout_tests accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "60000\n",
      "60000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jly09\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:190: RuntimeWarning: overflow encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Load data from Pickle file\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "train_labels_one_hot = data[4]\n",
    "test_labels_one_hot = data[5]\n",
    "\n",
    "img_size = 28 # dimensions\n",
    "num_of_labels = 10 # 0, 1, 2, ... 9\n",
    "image_pixels = img_size * img_size\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "simple_network = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                               no_of_out_nodes = 10, \n",
    "                               no_of_hidden_nodes = 100,\n",
    "                               activation_function = 'sigmoid',\n",
    "                               learning_rate = 0.1)\n",
    "    \n",
    " \n",
    "simple_network.train(train_imgs, \n",
    "                     train_labels_one_hot, \n",
    "                     active_input_percentage=1,\n",
    "                     active_hidden_percentage=1,\n",
    "                     no_of_dropout_tests = 100,\n",
    "                     epochs=epochs)\n",
    "\n",
    "corrects, wrongs = simple_network.evaluate(train_imgs, train_labels)\n",
    "print(\"accruracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = simple_network.evaluate(test_imgs, test_labels)\n",
    "print(\"accruracy: test\", corrects / ( corrects + wrongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
